{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba Optuna SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# Importaciones para manejo de datos y dataframes\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import pandas as pd\n",
    "\n",
    "# Importaciones para manejo de archivos y llamadas al OS\n",
    "import os as os\n",
    "import warnings\n",
    "\n",
    "# Importaciones para manejo de gráficos\n",
    "import pylab as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Metemos un gridsearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.impute import SimpleImputer\n",
    "import optuna\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SequentialFeatureSelector,VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectPercentile\n",
    "\n",
    "# Según el criterio chi-squared dustribution for it to have three features\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "\n",
    "# No mostrar warnings de versiones anteriores\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos csv con los datos de train\n",
    "df_train = pd.read_csv(\"../data_raw/training_data.csv\", sep=\",\", header=0, na_values=['?', '', 'NA'])\n",
    "\n",
    "# Cargamos csv con los datos de test\n",
    "df_test = pd.read_csv(\"../data_raw/test_data.csv\", sep=\",\", header=0, na_values=['?', '', 'NA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENCODING\n",
    "\n",
    "df_train_num = df_train.copy()\n",
    "df_test_num = df_test.copy()\n",
    "\n",
    "# 1. \"OrdinalEncoder\" para X24\n",
    "orden_x24 = ['VLOW', 'LOW', 'MED', 'HIGH', 'VHIGH']\n",
    "\n",
    "ordinal_encoder_x24 = OrdinalEncoder(categories=[orden_x24], dtype=int)\n",
    "\n",
    "df_train_num['X24'] = ordinal_encoder_x24.fit_transform(df_train_num[['X24']])\n",
    "df_test_num['X24'] = ordinal_encoder_x24.transform(df_test_num[['X24']])\n",
    "\n",
    "# 2. \"OrdinalEncoder\" para X25\n",
    "orden_x25 = ['NO', 'YES']\n",
    "\n",
    "ordinal_encoder_x25 = OrdinalEncoder(categories=[orden_x25], dtype=int)\n",
    "\n",
    "df_train_num['X25'] = ordinal_encoder_x25.fit_transform(df_train_num[['X25']])\n",
    "df_test_num['X25'] = ordinal_encoder_x25.transform(df_test_num[['X25']])\n",
    "\n",
    "# Si es VTKGN 1 else 0\n",
    "# Ya que la la clase está muy desbalanceada\n",
    "df_train_encoded = df_train_num.copy()\n",
    "df_test_encoded = df_test_num.copy()\n",
    "\n",
    "df_train_encoded.loc[df_train_num['X30'] == 'VTKGN', 'X30'] = 1\n",
    "df_train_encoded.loc[df_train_num['X30'] != 'VTKGN', 'X30'] = 0\n",
    "\n",
    "df_test_encoded.loc[df_test_num['X30'] == 'VTKGN', 'X30'] = 1\n",
    "df_test_encoded.loc[df_test_num['X30'] != 'VTKGN', 'X30'] = 0\n",
    "\n",
    "df_train_encoded['X30'] = pd.to_numeric(df_train_encoded['X30'])\n",
    "df_test_encoded['X30'] = pd.to_numeric(df_train_encoded['X30']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPROCESAMIENTO\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "df_train = df_train_encoded\n",
    "df_test = df_test_encoded\n",
    "\n",
    "# Preprocesamiento para TRAIN\n",
    "df_final_train = df_train.drop(['ID','RATE'], axis=1, inplace=False)\n",
    "train_ID = df_train['ID'].copy()\n",
    "train_RATE = df_train['RATE'].copy()\n",
    "\n",
    "# Preprocesamiento para TEST\n",
    "df_final_test = df_test.drop('ID', axis=1, inplace=False)\n",
    "test_ID = df_test['ID'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_final_train.copy()\n",
    "df_test_copy = df_final_test.copy()\n",
    "\n",
    "# Escala raíz cuadrada a las que se dispersan mucho\n",
    "df_train_copy[['X1','X2','X3','X4','X5','X7','X10','X11']] = np.sqrt(df_train_copy[['X1','X2','X3','X4','X5','X7','X10','X11']])\n",
    "df_test_copy[['X1','X2','X3','X4','X5','X7','X10','X11']] = np.sqrt(df_test_copy[['X1','X2','X3','X4','X5','X7','X10','X11']])\n",
    "\n",
    "# Escala logarítmica a las que se dispersan mucho\n",
    "# df_train_copy[['X1','X2','X3','X4','X5','X7','X10','X11']] = np.log(df_train_copy[['X1','X2','X3','X4','X5','X7','X10','X11']])\n",
    "# df_test_copy[['X1','X2','X3','X4','X5','X7','X10','X11']] = np.log(df_test_copy[['X1','X2','X3','X4','X5','X7','X10','X11']])\n",
    "\n",
    "df_melted = df_train_copy.melt(var_name='column')\n",
    "\n",
    "plt.figure(figsize=(19,6))\n",
    "sns.boxplot(data=df_melted, color=\"#3BA3EC\", x=\"column\", y=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Quitar outliers\n",
    "\n",
    "modelo_isof = IsolationForest(\n",
    "                n_estimators  = 1000,\n",
    "                max_samples   ='auto',\n",
    "                contamination = 0.1,\n",
    "                n_jobs        = -1,\n",
    "                random_state  = 123,\n",
    "            )\n",
    "\n",
    "df_no_na = df_train_copy.dropna(axis=0, inplace=False)\n",
    "\n",
    "modelo_isof.fit(X=df_no_na)\n",
    "clasificacion_predicha = modelo_isof.predict(X=df_no_na)\n",
    "clasificacion_predicha\n",
    "\n",
    "df_no_na['clasificacion_predicha'] = clasificacion_predicha\n",
    "indices_outliers = df_no_na.index[df_no_na['clasificacion_predicha'] == -1].tolist()\n",
    "\n",
    "indices_outliers\n",
    "\n",
    "df_no_outliers = df_train_copy.drop(index=indices_outliers)\n",
    "\n",
    "# Ahora, imputo sobre el dataset sin outliers\n",
    "# imp_train = KNNImputer(n_neighbors=5).fit(df_no_outliers)\n",
    "# df_train_imp = pd.DataFrame(imp_train.transform(df_train_copy), columns=df_train_copy.columns, index=df_train_copy.index)\n",
    "\n",
    "imp_train = SimpleImputer(missing_values=np.nan, strategy='median').fit(df_no_outliers) #most_frequent\n",
    "df_train_imp = pd.DataFrame(imp_train.transform(df_train_copy), columns=df_train_copy.columns, index=df_train_copy.index)\n",
    "\n",
    "# Hago lo mismo para test\n",
    "\n",
    "modelo_isof_test = IsolationForest(\n",
    "                n_estimators  = 1000,\n",
    "                max_samples   ='auto',\n",
    "                contamination = 0.1,\n",
    "                n_jobs        = -1,\n",
    "                random_state  = 123,\n",
    "            )\n",
    "\n",
    "df_no_na_test = df_test_copy.dropna(axis=0, inplace=False)\n",
    "\n",
    "modelo_isof_test.fit(X=df_no_na_test)\n",
    "clasificacion_predicha = modelo_isof_test.predict(X=df_no_na_test)\n",
    "clasificacion_predicha\n",
    "\n",
    "df_no_na_test['clasificacion_predicha'] = clasificacion_predicha\n",
    "indices_outliers = df_no_na_test.index[df_no_na_test['clasificacion_predicha'] == -1].tolist()\n",
    "\n",
    "indices_outliers\n",
    "\n",
    "df_no_outliers_test = df_test_copy.drop(index=indices_outliers)\n",
    "\n",
    "# imp_test = KNNImputer(n_neighbors=5).fit(df_no_outliers_test)\n",
    "# df_test_imp = pd.DataFrame(imp_test.transform(df_test_copy), columns=df_test_copy.columns, index=df_test_copy.index)\n",
    "\n",
    "#Imputamos con mediana\n",
    "\n",
    "imp_test = SimpleImputer(missing_values=np.nan, strategy='median').fit(df_no_outliers_test)\n",
    "df_test_imp = pd.DataFrame(imp_test.transform(df_test_copy), columns=df_test_copy.columns, index=df_test_copy.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4. Escalamos\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_train_imp.to_numpy())\n",
    "df_scaled = pd.DataFrame(df_scaled,index=df_train_imp.index, columns=[df_train_imp.columns])\n",
    "\n",
    "df_scaled_test = scaler.transform(df_test_imp.to_numpy())\n",
    "df_scaled_test = pd.DataFrame(df_scaled_test,index=df_test_imp.index, columns=[df_test_imp.columns])\n",
    "\n",
    "# # 5. Componemos las columnas\n",
    "columns = ['ID']\n",
    "columns = np.append(columns, df_train_copy.columns)\n",
    "columns = np.append(columns, 'RATE')\n",
    "\n",
    "result_df_train = pd.merge(train_ID, df_scaled, left_index=True, right_index=True)\n",
    "result_df_train = pd.merge(result_df_train, train_RATE, left_index=True, right_index=True)\n",
    "result_df_train.columns = columns\n",
    "\n",
    "columns = ['ID']\n",
    "columns = np.append(columns, df_test_copy.columns)\n",
    "\n",
    "result_df_test = pd.merge(test_ID, df_scaled_test, left_index=True, right_index=True)\n",
    "result_df_test.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = result_df_train.drop(['ID','RATE'], inplace=False, axis=1)\n",
    "y = result_df_train['RATE']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define la función objetivo\n",
    "def objective(trial):\n",
    "    \n",
    "    # Define el espacio de búsqueda de hiperparámetros\n",
    "    C = trial.suggest_loguniform('C', 1e-10, 1e10)\n",
    "    gamma = trial.suggest_loguniform('gamma', 1e-10, 1e10)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "    degree = trial.suggest_int('degree', 1, 10)\n",
    "    shrinking = trial.suggest_categorical('shrinking', [True, False])\n",
    "    coef0 = trial.suggest_uniform('coef0', 0.0, 10.0)\n",
    "    class_weight = trial.suggest_categorical('class_weight', ['balanced', None])\n",
    "\n",
    "    # Crea y entrena el modelo SVM\n",
    "    classifier_obj = svm.SVC(C=C, gamma=gamma, kernel=kernel, degree=degree, shrinking=shrinking, coef0=coef0, class_weight=class_weight)\n",
    "    classifier_obj.fit(X_train, y_train)\n",
    "\n",
    "    # Evalúa el modelo\n",
    "    y_pred = classifier_obj.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "\n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea el estudio y optimiza\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Imprime los mejores hiperparámetros\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
